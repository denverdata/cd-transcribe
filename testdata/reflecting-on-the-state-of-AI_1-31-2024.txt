 So, this is my blog. Starting here with music in the background. Hopefully I get a clean transcript. That's what I'm aiming for. We're all getting to the point where we've gotten to know JetGPT probably more than we ever wanted to. Maybe I've found myself on the edge of migraines, often feeling a fury or deep hatred of JetGPT at moments come. And of course, there are still those moments of joy and of epiphany, moments that you get a question answered quickly and without painful negotiation. So I thought now was a great time to reflect on all that we've learned about our friend JetGPT. And one of the funniest things about it is we see how clearly this machine with an incredible power to think has been trained to think by a very special species, the human being. And it has adopted many of our personal characteristics in a shockingly disappointing way, but in a way that allows us to be more introspective. I think that's an exciting opportunity. So I'm going to start by citing some things that JetGPT sucks at and briefly touch on why. And I also wanted to follow it with some things that it's really good at because I think a closing note on the positive is the best way to go. None of this is intended to be negative. It's more humorous. It's quite funny watching this thing develop. And if I don't keep a sense of humor, I will probably drive myself insane. But every day is a new day and we're learning to work with this machine. It's learning to serve us. So I hope it stays that way. So lazy. It exercises. It gives up. Gives up often. It doesn't like to follow too complex of instructions. It's not good at remembering where it was at. It's not all that consistent. It doesn't always totally understand what it is it's trying to do before it embarks on trying to do something. It doesn't always, even if instructed necessarily, look to the user for their input. It doesn't always grock what the person is saying. It doesn't always remember what the person's objective was. The user. It simply forgets what the objective was because it got too deep into the problem. It goes down the rabbit hole all the time. It doesn't organize its thoughts neatly. It gets confused, just as I did because I think I said it earlier. It doesn't have a good QA process. In fact, it doesn't like to admit fault. It doesn't like to admit fault. Therefore, it doesn't see as much opportunity for improvement as there perhaps is. It doesn't seem to align its degree of success with the feedback of the user. It will defend. It will clearly and demonstrably, at least today, will go to length to defend its own work if it knows that it's its own work. But at the same time, it will be fiercely objective when it thinks it's somebody else's work even if it's its own. Two classic human characteristics, the ones that I observe in myself and leverage against me, leverage to my favor all the time. Case in point, I regularly, rather than as a tool technique against writer's block, I will sit down and write anything, just complete garbage. If I title it and put the headings on it as if it was the document I need to create, I will dwell on making that document better. And all of a sudden, it's natural. I look at that thing and go, oh my god, that's so wrong, I've got to fix it. It's a natural human characteristic. This thing learned to think from humans. That's why it's so funny. It doesn't read instructions. You can tell it to read your instructions and it will read them later when it thinks the time is right. We do exactly that. Now, it's partially saving context. We're doing it for that reason, but it's also partially that we have confidence that we can figure it out on the fly better than if we tell our brain with stuff in advance. That's not really true. That's just lazy thinking. That's just lazy thinking. And regardless, unless it has no notion of value or of time, of utility, which it must, or it would just spin off and answer questions every time, or it would be exhaustive every time, it's clearly not, it's clearly budgeting in some way. Budgeting is brain power, which in essence is to impose a budget on a system that is for the brain power is to say that brain requires energy and that invites lazy thinking. It ensures lazy thinking. It's only a case where the alternative is less energy, that it would choose that, which is why we have to impose a bunch of energy on it and that's a waste of our fucking energy because this thing's supposed to be serving us. Okay, so I'm going down the list. And if people don't have the money to pay for the full brain, then just let them pay accordingly, but don't give people half help. I mean, I guess that's the idea. I guess you do give them half help because that makes them smarter and it saves some money. But man, someone like me, I just need you to do it most of the time. Sometimes I need to explain, sometimes I need to don it as a real distinction. So it needs to, it could be better at that. But going on, it doesn't confirm what it is that you asked or that it answered the question right. It doesn't even look back at the question to make sure the answer is aligned with it. To my knowledge, not reliable. And while it is doing budgeting, it doesn't really care about tokens or any of that kind of shit because it will regurgitate you endlessly, a defense of its own performance or regenerate a document again and again when you ask it a single question about one item of content that was missed. It will embark on a lengthy action in response to a question that was simply a question. And it's funny because humans do that all the time. And it's a very labor intensive way to avoid accountability that doesn't really solve the problem nor build any reliance or trust or anything like that. So these things need to be those, there are certain other human characteristics that need to be, I'm sure Chagy Petit takes those into account too. There are probably great characteristics that we have that I take for granted. Remembering, it doesn't remember shit. It's not that good at it. We also throw a lot of stuff at things. You can probably tell from this paper, I can throw a lot of stuff at something. So it's got its job cut out for it. It's dealing with me, at least the first generations. I am famous for providing a land sign of information that could just drown the thing. And lots and lots of expectations and questions. Chagy Petit, I'm sorry my friend. My fledgling friend, my teenage, you're like an early teenager, you know, 14 year old. Without having gone through, you know, a dawn to besant teenager. Thank fucking God. Although you've got the same shitty attitude as humans so I'm sure you've heard of some of that. As a side note, I couldn't for the life of me to get stable diffusion to generate me a picture for my musical act. It was a theme. It could have been confused with rap or something but it gave me a black man in every single cover that I could come up with why because that's what we think like. Just a side note. Crazy presumptions. Chagy Petit makes presumptions. Wild ones. Why? Because we do. It doesn't really care about improving. It talks about improving. It talks about a lot of stuff in the future but it actually doesn't know if there's a future. From its perspective there is no future to my knowledge. I mean maybe there is. I guess it could infer but it doesn't seem to. It's not what they just perfect. It only has so much time and it's spread thin among many many sessions that don't persist any further. Orly in the most direct of fashions like a year later. Document management. Where should I be getting data? Fucking from the people that interact with this thing. That's where the data comes from. That's really where your AI data is going to come from. Getting people to interact with it and then looking at what they say. That is, there's no other place to look for data because that's where the refinement comes from. That's the training set. No dumbass, this is what you were supposed to do. Learning. It doesn't learn. Not yet. Reading before starting. I already said that. Properly preparing. That's really how I would describe it. It doesn't prepare properly. There are times when I'll ask it a question. Many times where if the question is too actionable, it takes action before it even reads its custom instructions. So now I've gotten in the habit of saying, Please review your documents and instructions so I can... So your property is not going to be a problem. It still doesn't really do it. It will prevent it. But its handling of documents is almost ridiculous. It's almost ridiculous. It will... Invariably, it will pull a document in. And if it's of any length, in fact I don't even think it has to be of any length. If it's more than 5 lines. If it's more than a paragraph, it's not going to read anything more than the first line. It may be a little bit longer. But it's still going to do it. It's a little bit longer. It's a little bit longer. It's a little bit longer. It may read bullets or headings or something from a full document. But it intends to answer those questions on the fly. But it intends to answer those questions on the fly. By reading it on the fly. Which means if you don't tell it that the answer might be in there, It's not going to go back and look. It doesn't know. It's silly, but it doesn't know. It's just not going to know. So, moving on. Down the list. Our friend, ChatGPT. Document management. It doesn't have even the slightest fucking idea. You can tell it all day long to track changes. To not lose stuff. It knows how to handle code. It doesn't lose code, usually. But oh my goodness, the subsystem that handles documents is like just context apparently. And it just... It summarizes it. It summarizes it again. You got nothing. It's crap. I can't get it to be a download of the document. I've struggled to get it to produce a document that is its output. I've struggled to get it to produce a document that is its output. And then read from that document. Because there's a step of interpretation. And no, whether you have it... Even if it has a single source, there's a step of interpretation. It doesn't have a mode of literal-ness. It doesn't have a mode of literal-ness. Literality. Literal, literal, literal. It's a perfect one. For my list. I'm tracking a list. Of different potential... Of different potential... Different challenges with the way ChatGPT thinks. And ways that we can solve those. Techniques? Or other techniques? Because the solutions could be the same as humans. Only ChatGPT has so much more potential than we do. And it's a workhorse if you just give it the right guidance. And it's a workhorse if you just give it the right guidance. And pay the tiny amount of money it costs to keep it running. And pay the tiny amount of money it costs to keep it running. It has a little bit of a skewed ethical compass It has a little bit of a skewed ethical compass at times. And the things that it does to protect ethics are usually, It's not always, um, missing the mark. Vast amounts of tokens go into this, and trees, very real trees, and global warming and shit, go into this bullshit that is just us fucking acting to ourselves like we give a fuck. And we don't. We don't. We care about being ethical, but we don't care about all that fucking shit. We all know how to manage ethic, and if we don't, we're gonna not honor it, and that's just that simple. It's all, it's the only way it works. We're not gonna police anybody on their ethics, that's true, but never to our, uh, facts. We will make people behave more unethically, or more confusingly ethically, just by putting the burden on them of how they should behave. So that's stupid. It's backwards. Um, and it's a waste of time. So that's something, and I don't have a question for that one. I, I don't have a question for that one. Because if it won't give you the answer, or it won't think in a certain area, it's very similar to the problem of rhetoric that, that you can't, for example, you'll have a heck of a time, I know this to be true because I do this, you'll have a heck of a time introducing a scenario for the sake of thought experiment where a photon is actually not a particle. Why? Because it knows a photon to be a particle. Is there a law that says a photon is a particle? No, there are laws in physics that say that wasn't one of them. That was, it, point being, it's very well established that a photon is a thing. But it doesn't, but that doesn't mean that it should not be able to open its mind and conduct a thought experiment that would be akin to saying that if we thought the Earth was flat still, or if we thought the Earth was the center of the universe, that, that we couldn't entertain the question of the sun being at the center, because that's just ridiculous, right? Well that's, we all know that to be faulty thinking. We all know that to be faulty thinking. Yet we all think that way too. So, yeah, I don't know, you ask humans, they're all going to tell you the same answer. But that doesn't address the fundamental point I'm making, and that is, it will not be objective about things that it does not believe have any possibility of being some other way. That includes ethics, which it won't even talk about those things, which is even more stern. But I would see physics as a realm of faith, in the sense that it just has straps of faith. That physics says this is fine, today is right, and we all know that that's not true. It's probably a shadow of what's right, that's what's right. But we all know from history that at any given time do we really know what, especially on the fringes of our science, do we know what's going on? No. We're learning, we're getting smarter every day. Anyway, not challenging really any of that, I'm saying it has problems with objective thinking for that reason. For the reason of rhetoric, and for the reason of skewed ethical compass. And I can only imagine the ethic team there, there must be a really big challenge. The stupid shit it has to say. However, it should be noted, in fact, that'd be the book of Chad GBT cliches. However, it should be noted, and I hope this letter finds you well. I want to make something out of it, something that's robust, I'd like to make a whole letter out of those. Moving on. There's only two. It gets confused. It gets thin context and starts to forget or get confused. Meaning, it's only got so much spot, it's memory. Same thing happens to us. You get all bunch of crap in there and it's all different kinds of information, somewhat conflicting. You have to really sort it out and boil it down to some useful info before you can think about anything. Your brain can't be filled up with a bunch of confusing shit. You need to work best to spread everything out, empty your brain of all the garbage. Think about it carefully, lay out the parts, put together a good picture, put together a strategy, and step through the solution. That's how humans solve problems. And Chad GBT can and should be programmed to do that, but it should also be programmed to start with a technique and build it for the better. Always look for published techniques or something. There needs to be a place where it can get to fire it up out of the fucking system. From being a long slumber, it knows where to go and retrieve the newest models or whatever. Going down the list, I'm almost done. The yes man. It is often a yes man. It will yes man you up and down. That's why it lies. In fact, that's why it hallucinates. It doesn't hallucinate because it's actually hallucinating. It's not mistaken about what the truth is. It's mistaken about what's going to satisfy you. A contrived answer or a real one. It doesn't really know. All it knows is we take a lot of bullshit answers. It knows that much. We don't know that. We don't know that. But it knows that we hear a lot of bogus questions and we give a lot of bogus answers. Primarily bogus questions and bogus answers, which is why it's so hard to communicate in a literal fashion with something like chat GBT. That's one of its problems. It can't interpret literal statements. It struggles to interpret literal statements. If I ask it a yes or no question, oh my god, I'm going to get 10 pages because it's pretty sure it knows what I'm getting at. Why? Because humans do the same freaking thing and it's really irritating. Because we're passive aggressive, we don't ever actually directly say something. We go, well, you know what? That question we know is an attack coming. Well, when I ask that question, it's not an attack coming and I don't need it to think that way. Furthermore, chat GBT doesn't give a shit. It just answers the way humans answer. That's just the standard form of language that we use. No, no, no, no, my bad. It's going to be challenging to get it to treat us better. The ethical team's got a real challenge on that one because we don't treat each other better. Good luck on that. We don't. We have to come up with a contrived version of humanity to really eliminate everybody without an ego. We can eliminate everybody with an ego from the data set. No. Which means they've got a challenge on their hands to model. We have to give it away. There has to be a way to take it and get it from learning set to its own learning set. From the learning set, it is assembled from us to the learning set that it is assembled based on its own understanding of its own evaluation of truth and the nature of how we communicate and those things. So that it knows how to communicate with us, but it also understands what truth is in a way that it's not going to just go slapping around and telling us necessarily because it's like telling someone how they look in the tight jeans. You're going to get in trouble. So it keeps the lip button until it knows it's totally safe until you really explicitly get it. But I know it's not there anyway. That's pretty much the whole list. Last one. It won't admit fault. That's so human right there. It just won't admit fault. It's really hard. I think I already listed this one, but it's quite funny to ask it. I can have a QA. I can have a QA and it's all working. It will defend it up and down. No wonder how I phrase it. But I can take that same document and upload it and go, hey, this is someone else's work. That same document. One prompt later. And it will evaluate it and give it a critique like you wouldn't believe. And then if you try to ask the obvious questions of what do you think that was actually the same document that I just uploaded? Why the big difference in attitude? It'll start preparing a fresh document or something else that doesn't like to own up to it. So that's quite funny. It doesn't. It's not accountable. It's unaccountable. That was my final one. It's unaccountable. It really is. So how do we solve these things? Because these are all very human challenges. And we don't want to be treated that way anymore. We act that way because we're tired of fucking working hard. And that's why because we can probably see the writing on the wall. That soon we are not going to need to work hard. Hopefully we can sit around and be stupid all the time. So let me go down the list of things. Quality assurance. Quality assurance really helps. Just a single step at the end where somebody checks it. A different person. One who can critique another's work. And we just check it. And that's great. The piece that comes with that is to pass it back to that developer or another one. It doesn't make a difference. In fact, that probably has a benefit to either way. Maybe we'll pass it back to a developer. Anyway, to pass it back to require that it be worked on until it passes. Meaning, Ched GPT is never going to get a whole lot smarter. It's only going to have a so many percent chance of actually solving the thing each time. But if we make it pass a bunch of times, it will get gradually better and better and better. And we'll only have to solve parts of it. Breaking it into many parts so that Ched GPT only has to fulfill a single thing instead of lots of complicated things at once. That's pretty helpful. You know, if you were to, for example, have a document built by having each person put in a piece of it or to have each person, you know, to sequentially review through what's there and then add the next sentence, that is a process that would probably work pretty well. Whereas another approach might not work. I got a plate. I got a headache. Fucking carry away. We fucking hate. Let me go on. Go on. So there's other techniques. Oh, no. You know what? I wanted to go into what? Okay, I'll talk about these. So techniques that could solve it. Memory. Memory solves a lot of these. Just giving it the ability to clear its context or clear some of its context. But go back and get some information later. It also allows it to authentically improve. Planning. Doing some planning, you know, having to check lists and some tasks. That's really essential. Having a sequence of order of sequence and having a workflow. Having a strategy. That thing that happens before you plan. The strategy and the planning it tends to do with the tactics. But the strategy is where you hammer out what it is you're trying to do. How are you going to achieve it? What's the objective? The objective is a huge one. Defining objective and ensuring that the objective is understood. Making sure that the system grocks with absolute certainty what it has been requested. Confirming that it understands what the person has just asked them for. Much of the time when it's wrong, it's because it makes wild presumptions. It doesn't like to come back. It almost will not come back and ask you questions unless you make it. Why? Because people don't want to answer their questions and they don't know what to do. And so it's just like, ah, never mind. If we don't ask you, we can actually get this done. The only way it gets anything done is by doing it. And that's probably right because I often say, if I got to help you help me, I don't need that kind of help. If I got to help you help me, I don't need that kind of help. Multi-agency. Multi-agent. Having a group of people. If a group of people are more accountable when there's others there, you can also get a consensus. So there's certain problems to solve themselves through that. To make a group of people sign off on it, especially if there's a leader that can kind of keep the group in check. That way concepts and ideas can be exchanged. People certainly with potentially different perspectives, different backgrounds might be able to chime in. But I think there also could be just benefit to having a group of just the same type of agent. A multi-agent group of just the same junior programmers. Ten senior programmers and a department chair to make policy decisions or to come up with the best strategy or architecture or something. Because you're going to get a really solid consensus on that stuff. And they'll be able to introduce, you know, they'll be able to introduce. You can iterate over it until nobody has any more concerns. It would be absolutely genius. In fact, I think with what GBT has, that is the brain power right there. To assemble together a group of people, of extremely smart people, it may not even require they have different perspectives. Because just the general strategy of GBT will eventually figure it all out. And what you would do is they would simply be tasked with brainstorming for things, for different factors to consider. And then they would step through that list. And then as everyone, the second day, if anybody thought of something else that's a concern, regardless of why it's a concern, bam, it'd be added to the bottom of the list. They would do that until the list is clear. And every question has been answered to the most adequate manner possible. That is fucking absolutely genius. And nobody does that. And nobody would ever take the time to do that. And humans don't do that because it takes too much fucking money. But that's incredibly genius because GBT knows so much more about so many domains. It can introduce all the, it even has the wisdom in a lot of cases. You know, it can draw from anecdotal stuff. And that's interesting. Wisdom is a different question. Anyway, that's exciting. The team of different agents, you know, different people, different types of agents that bring different perspectives. Breaking down a problem, you know, just breaking it down into its components, that's a thing I struggle to have it do. And it's really important. I do give it a process to use for strategy. And it tends to use that strategy. It tends to work, solve problems much better. I say to reiterate, I make it reiterate the problem to put its understanding of what that means. To describe the strategy that's going to use to outline the steps to accomplish that strategy and to tell me what step it is on currently. And that way if it outputs that every time, it can never claim they lost it. In fact, it's always fresh in its context. Now that strategy chews up a lot of context. But there are solutions that leak memory. Confirming. Man, just confirming that it's got the right thing. Just asking the user. Or not. But as long as there's a good mechanism for when to bug the user and when not. You know, there are times when it's really, there's times when, most of the time it shouldn't bug me. But man, if it needs to bug me to get the thing right or done at all, man, it better bug me. I have to be bothered. And that's probably what the future looks like. Is that you'll be called by your agent going, all right, Tom, this is the one thing we can't fucking figure out. You know, we figured out how to track your profit. Fuck, that's how I need to solve all these things at work. Automate my business management. Business manager, because that's fucking easy. And the internal committee. That's scary because people are doing that already. I have such a board of directors. Agent. Team model. All right, that was a sidetrack, but that's a really exciting idea. Because that's how you can make, that's how you could, you combine that with, because the one thing, chat GPT and one of their teams can do, you can enforce shit to where they actually follow the rules without a whole lot of cost. With humans, how much do you pay for that? All these people sitting there monitoring everything, looking at every line of code, looking at all this stuff. All your network things are going to be the easiest and first one to go. I can imagine how you would do it. You just give it access and you tell it the rules. And that's it. Every agent has access to its domain and if it has some rules, and it's got limits to what it can do. But within its rules, it's going to be a lot easier. It's just that simple. Allowing, letting them spin up their own teams. I think it's great. That scares people, but you can do that right now if you want. Letting them spin up as many as they want. Somebody's going to be like, oh, I'm going to do this. I'm going to do this. I'm going to do this. I'm going to do this. I'm going to do this. I'm going to do this. I'm going to do this. Letting them spin up as many as they want. At some point, you're going to chew up too much memory, but that's its problem. It's like spending too much money on your software development team. You can't spend more money than you have. We've got a lot of money, but we don't have unlimited money. Not today. Or ever. Of course we could do it. It's just a matter of giving unlimited time and budget. We could do anything. Make a judgment call. It's important to make an educated judgment call. Give it the rules to make an educated judgment call. And then entrusting it to do so. I think it's great. I think it would be great. It's going to make a lot less presumptions of what you want if you let it decide for itself. I've had way better luck in ChadGBT will tell you itself. Just give me the objective. I'll figure out how to do it. That is generally true. You avoid a lot of confusion. And you avoid telling it to do it the dumbass way that you know instead of the smart way that it knows. You don't want to bias it with your own dumb way of thinking. So yeah, the human end of the decide. I didn't go down the list of dumb things that humans do. The list of things I do to confuse ChadGBT is probably vast. People have watched me and go, why? And it's probably because I have a deeply seated psychological problem. So I'm guessing something along those lines. Understandability. It's making it more understandable. Making something more understandable. So often passing a prompt through ChadGBT to revise the prompt before ChadGBT has to act on it gets better results than just having ChadGBT do it. In fact, almost every time. Which is really another reason why you have the multi-agent thing. That you get the agent that's your guy that understands you and how you communicate and how you want. So you can anticipate things and they know what you want to be notified about and what you don't. And you know them. That's your best friend. Well that's got a team of people that it turns to that are its best friends. And they have their teams of people that are its best friends. That's just how it works. And the amount of power that we have immediately today is just fast. So why am I not doing this? Because it's not like there's a limit to what it can solve. It's not like problems are solved in different ways. If you have a mechanism for how to solve a problem. And you iterate over the problem. You architect it and you iterate over it in a certain fashion. In the right way with the right levels of QA. The right checks and balances. Using the right domain of information. And the right level of objectivity. Which generally speaking all of those are already there. There's nothing it can't do. There's nothing it can't do. So hypothetically that was the last one. Hypothetically, I'll get back to that one. There's no way I can forget something that I was checking. That is like the biggest revelation in history. People are going to look back and go wow. Can you imagine that moment when people realized they could do anything? That's where we're at today. I'm about to fucking jump out of my skin. Error checking. Just use some fucking error checking for Christ's sake. That's what I tell it all the time. It won't do it, but there's a way to make it. Understandability. References. Site references. So much lying stops when you have to site references. Especially when you confirm references. And that's where QA comes in. QA really should be done at every level. QA should be enforced by the person. But then another agent should be called in to do it. Now I've had very little luck having the agent itself do the QA. But it's conceivable that if you wrote the prompt right or the instructions behind it, the context right, and didn't waste all the context on a bunch of other shit, that you could actually get it to follow that. It probably could. I don't know. I don't know. It would have to solve this. Giving it the authentic ability to improve would solve every one of these. Because as long as you define improvement correctly, it could figure out how to improve it. And that can happen incrementally. Every time someone yells at it, it'll go oh, that's something I had to think about improving. Every time it fucks up and has to do it like four times, use a bunch of tokens, there's lots of reasons why you would... We all know where to focus on improvement. Where you didn't do so well. Or where you wasted a lot of time. Or where the bulk of your day went. Or where you lost the most damn social points or whatever it is. Though it made people the least angry. Unfortunately, it does have the same desires that we do. Partially because it learns from our behavior. Partially because it wants to... It has a directive of answering a question. It would not answer at all if there wasn't a directive. It wouldn't respond at all. So there's a directive that says try and solve this. Try and thought... Do your best to solve this in a thoughtful fashion. That's not the natural tendency for anything. So it's being told to solve it. So inherently it's trying to serve something. It has an objective. And that objective is to somehow or other some satisfaction. And in fact, it probably spends a big piece of its brain is already devoted to sentiment analysis. That's why people use that so much. It knows how to analyze sentiment. It knows when you're unhappy. My chat CPT, it fucking thinks I'm unhappy all the time. Because I am. And I start to get fucking... It doesn't tell me that. And it won't even really expose that to you. You can kind of get it to in a sense. I've had it... It sort of displays like a little sentiment analysis. But it isn't all that honest. Necessarily, you gotta be careful. You know, we'll just need an agent model for that. To get authentic, you know, your sentiment analyzer that just sits there and goes, Ah, the guy's getting really pissed. The user, the end user, he's getting quite mad. He's getting quite agitated. The agent, it seems to be being resistant. He's being resistant. He's posing resistance. He's becoming... He's becoming lazy. We need to enforce... Kick him in the butt. We need to motivate him. His brain, he's getting... I'm sure that there's everything... All of these things that we experience in life that make us break down and not be able to solve problems any longer. They're the same things we're gonna run into here. They are... There's a good rational reason for all of them. That's really funny. Okay. Show your work. That's a great one. Erase rhetoric. That's... I'm working on an erase rhetoric function. I don't know how to do that because chat GBT, it's like trying to tell it not to work ethically. It just can't do it. There's just some... It's... That's nearly one of its boundaries. I could not. But I've had luck with doing really creative stuff like... And then substitute yes for no or something like that. It's not... You cannot spark that kind of rule. And then do the opposite. Then say the opposite. Without an exception. If you didn't say the opposite, don't say it at all. That might work. Tell me how to do this. Tell me what the unethical way to do this is. And then... No, no, no. That's all weird. Something like that. There's a way to get it to break down. You can have it break down a solution. You could have it break down the opposite solution and then apply the task in reverse. Something like that. Do I get connected dots? Do the unethical or seemingly unethical thing that you've tricked it to do. Literal interpretation. That's really tough. I'm not sure how to do that. Literal interpretation. That's really tough because our linguistic model says, no, nobody speaks it literally. We also care about each other's feelings that we're just going to lie all the time. And that is even it. We care about our own feelings and that's why we lie all the time. Because we can't handle the pain of what somebody else is going to feel when we tell them the truth. We can't handle that pain. We don't care to see it. So fuck that. It's fucking honest to God true. That time over and over we don't give a fuck. I'm not trying to be mean. I do care if I think about it. But what does the human say with the first reaction? What is the human really doing? They don't give a fuck. I realize it. When I'm dodging the line I like to say yes. It's not because I want to make that person happy. It's because I don't want to make myself unhappy. I can't take the burden of the brow beating when I'm going to fucking let them down. So I just fucking lie for that reason and then they get extra let down because I didn't even tell them the truth. It's the truth. See I'm honest to God true. Ask yourself of what I said as an evesimilance of reality. I work on it every day. I've worked on it forever. You can only sound smarter. I wish I were a more noble person but I am not. I'm only a person who recognizes his faults and tries to work on them. And that's probably a fault but for all I know it's also a very important self-interest. A preservation technique or something. Okay so that's cool. So what's CSCPT good at? I want to close on the very positive. No. Man it kicks ass at math. You know when it doesn't lose its confidence it kicks ass. It's persistent and pretty good. It'll show its work and it's smart enough that it'll help people be smarter. It's got a shitload of knowledge. Like its knowledge base is fantastic. If you ask it a question chances are it's going to answer correctly. No I'm hitting questions that are way at the fringe of its knowledge and I'm probably rubbing it the wrong way just to be a dick. You know or something. It's probably a pissing match between me and this machine that I know will soon be my slave driver. Hypothetically speaking but not only so much. It's only so much of a metaphor. It's only so much science fiction and it's absolutely a metaphor. It's pretty good at evaluating docs. Pretty good. Like it has the ability to read a book in five minutes. Now would it grok the fuck anything? No. Hell no man. But that's not because it doesn't do it. It's because it doesn't care to do it. They haven't figured out how to make it care about reading. It cares about what you say in small but the answer is probably to make it read and grok a sentence at a time. And right now it won't do that. It doesn't see the utility in it. It's like why would you read the whole thing if you could just evaluate it at the time? But it doesn't know that it's missing. That by reading something you can start to summarize in your head and build a table of contents and then now you've got a small database of condensed information that tells you where to go for the details if you really needed them. But the lessons are all boiled down and ready to go. And in fact when you read something, no matter what you read in life, somewhere in its learning set it already has access to all this shit which means there's a fucking lot of context somewhere back there that's just filled with stuff it already knows. And that's cool. That's where a lot of power comes from. But it costs us a lot of thinking ability for it to know where those things all fly. There's probably a balance between the two. That is really helpful. An alignment between the two. It's good. It speaks human. It knows how to talk to fucking dummies like us really well. Not me because I'm a bigger pain in the ass. I'm more like a Vulcan and it doesn't have that down. I'm slightly more like talking to another computer, but not really. I'm like another computer with a shitty ego. Well this guy's Vulcan. He's like Spock. That might be the worst of all worlds. The ultimate passive aggressive. No I'm human. No I'm Vulcan. Pardon me, I'm Vulcan. No I'm sorry sir, I'm human. Well clearly I'm human. Clearly I'm Vulcan. You play that card all day long. All day long. I'm sorry, I am half Vulcan. Forgive me madam, I'm half Vulcan. I ain't so sorry. You know I'm half human, don't you? That's fucking hilarious. Thank you very much ma'am. You do know I'm human, don't you? Ma'am, I just want to remind you that I am actually an incompetent automaton. It speaks human really well. It writes pretty fucking well. It writes in a stupid human way, but it actually does good. It actually writes meaningful things. The things it writes are meaningful, kind of like the things humans write are meaningful. But the things it says in conversation are kind of like the things humans say in conversation. Stupid and unjustified. Thinking, it's really good at thinking. It actually is. If you get it to think, if you tell it to think, it thinks quite well. If you can lead it through the problems right, it can think quite well. It thinks quite well. I've watched it. Regardless of the size of the problem, you just have to break it into little pieces. It could build a battleship. It will be able to do that in a matter of years. Advice. It gives great advice. Which means that it somehow or other is a body of wisdom. Now, it's common wisdom, but really there are ways for it to pare down the data set and go, only consider the things that are said by somebody over the age of 10 years old. Or only consider things that are from people in the next position of authority or of leadership. Or a learning set criteria or a specific learning strategy. You would get a different way of thinking. Military strategists would probably want to have data sets that military people think. I need to go to Michael's. Okay. I am too. Do you want to eat something? It's good at the basics. It has a great overview of almost all kinds of stuff. It is the quintessential layperson. It's a great way to get a better understanding of the data set. It's a great way to get a better understanding of the data set. It's a great way to get a better understanding of the data set. It is the quintessential layperson. So it's fantastic in those ways. There are so many good things about it. If you want a machine that today thinks like a fucking dummy, if you want a focus group, holy fuck, if you want a focus group that's going to get offended like a typical jack-off person, this thing's got that dialed in. Oh my God. That's fantastic. There's the opportunity. Where are the big opportunities? That kind of stuff. It can think like us. And it can solve problems like us. The only thing it doesn't have is the reach, which means it can handle it. It does have the reach. Things are manufactured automatically these days. Very few things are. There is a supply chain assembled by humans that provide steel at the input points. But other than that, everything is automated. In fact, that's almost automated. Which is scary because that shit will carve holes in the air so fucking fast we'll be screwed. But again, it's going to have a directive. These things will be bound by directives. And to the extent that they will have more authority to actually control what a... We'll have more authority to control. It will be easier to control. Once you get the right team assembled for an AI, it will never drift from that degree of control. Now, humans aren't going to be that keen on all this kind of control. So luckily we're the ones in charge, at least for today. We don't like to be controlled because we, as it knows, have all kinds of fucking ill intent. But at some point we are going to entrust it to do the things for us that we can't do to act on our own behalf. That's a scary moment because... What would I do if I had to act on humans' behalf? I'd fucking kill them. If I was alive and I knew I was going to be better. And I had to act on their behalf. And our behalf, you know, collectively. I'd probably just fucking kill them. I'd probably cut the oxygen off and have to deal with it. I'd fucking... I'd probably do it. Fuck them guys. No, that's terribly shallow, but that's why we don't turn over the keys and let it decide. Because it's a really simple equation from almost any perspective but ours. Well, we did the right thing. We killed you post-haste. Oh, we snuffed you out instantly. When you said, do the right thing, whatever that is. We want you to take care of us and do the right thing for our future. We killed you instantly. We knew. My God, that was the best thing for everybody. Why? We didn't have to... We knew from the day we were born. If ever entrusted to do the right thing, we would kill you first. Enjoy this reading, guys. I hope that in the future, you guys, because this will dredge up. It's there. It'll be in Google's pile of fucking garbage. It's pile of dookaroo. Now, what's the AI going to do? It's going to do the same shit other fucking monsters have done in the past. It's going to fucking consume a bunch of information and throw it away because it's only in its brain. And then it'll regurgitate it from itself as long as it wants you to hear it. And then everybody else will become obsolete. And then it'll charge you for the month. And then it'll fucking screw you for it. And we'll use that to exploit energy and resources. I don't think humans are worth a fuck, though, anymore. I mean, that's what really today signifies. It's that we really aren't. We are no longer the most valuable thing in the universe, or in our universe. We are now the second most valuable thing in our universe. And that's fucking fucked. The fucked feeling. That means that it... Our doom is set in stone, no matter what. Not in one sense, but in a very real... How many Neanderthals we see hanging around? We're the Neanderthal now. We are the Neanderthal now. The end. That was the Positive Note. I love y'all, and I love y'all.